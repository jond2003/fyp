import pandas as pd
import torch
from transformers import BertTokenizer
from classifier import BERTClassifier
from global_vars import BERT_PATH, TEXT_GEN_PATH

# initialize classifier
def init_classifier():
    bert_model_name = 'bert-base-uncased'
    num_classes = 20
    model = BERTClassifier(bert_model_name, num_classes)

    # load trained model
    model.load_state_dict(torch.load(BERT_PATH))
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    tokenizer = BertTokenizer.from_pretrained(bert_model_name)

    return model, tokenizer, device


# evaluate the texts generated by the LLM using the BERT classifier
def validate_results():
    print("\nValidating LLM results...")
    # load results data
    df = pd.read_csv(TEXT_GEN_PATH)
    targets = df['target_idx'].tolist()
    bloggers = df['blogger_id'].tolist()
    texts = df['generated_text'].tolist()

    model, tokenizer, device = init_classifier()

    num_bloggers = len(set(bloggers))
    text_concats = {}

    # concatenate bloggers' texts together
    for k in range(num_bloggers):
        t = texts[k]
        blogger = bloggers[k]

        if blogger not in text_concats:
            text_concats[blogger] = t + " "
        else:
            text_concats[blogger] += t + " "
        

    predictions = {}
    correct_preds = 0

    # individual texts
    for i in range(len(texts)):
        target = targets[i]
        text = texts[i]
        exp_blogger = bloggers[i]
        pred_blogger = predict_blogger(text, model, tokenizer, device)

        is_correct = exp_blogger == pred_blogger
        if is_correct: correct_preds += 1

        if target not in predictions:
            predictions[target] = [is_correct]
        else:
            predictions[target].append(is_correct)
    
    # concatenated texts per blogger
    correct_concat_preds = 0
    for blogger in set(bloggers):
        text = text_concats[blogger]
        pred_blogger = predict_blogger(text, model, tokenizer, device)

        if blogger == pred_blogger: correct_concat_preds += 1
    
    print(f"Predicted blogs with {(correct_preds*100)/len(bloggers)}% accuracy")
    print(f"Predicted concatenated blogs with {(correct_concat_preds*100)/len(set(bloggers))}% accuracy")


# predict writing style ownership using fine-tuned BERT classifier
def predict_blogger(text, model, tokenizer, device, max_length=128):
    model.eval()  # set model to evaluation mode
    encoding = tokenizer(text, return_tensors='pt', max_length=max_length, padding='max_length', truncation=True)
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        _, preds = torch.max(outputs, dim=1)
    return preds.item()
